\section{Постановка задачи}

Рассматриваются два игрока - Студент, далее обозначается \textbf{С},
и Преподаватель, далее обозначается \textbf{П}, которые имеют противоположные интересы.
Критерий интересов составляют две велечины, первая из которых является эффективностью
работы \textbf{С} в научной сфере, а второй его эффективностью на подработке.

\textbf{С} выбирает долю $x$ рабочего времени, которую он тратит на подготовку
диплома, оставшееся рабочее время $1-x$ он тратит на подработку. Считается, что производительность \textbf{С} при любых занятий падает с увеличением 
отводимого на них времени, эффективность труда \textbf{С} зададим функцией $\sqrt{x}$
и $\sqrt{1-x}$ соответственно. \textbf{С} может распределять свое время между двумя 
видами деятельности, т.е. имеет множетсво стратегий $x\in X = \{0, 1\}$, 
причём он может использовать смешанные стратегии.

\textbf{П} выбирает - отностится к \textbf{С} требовательно, способствую 
написанию диплома и мешая подработке или же не обращать на него внимания не 
мешая подработке и не помогая с дипломом. \textbf{П} имеет множество стратегий 
$y \in Y=\{1, 2\}$, причём тоже может использовать смешаные стратегии.

Получаем следующую вектор-функция выигрыша:

\begin{equation}
	F(x, y)=
	\big(f_1(x,y), f_2(x,y)\big) =
	\Big(
		\frac{y\sqrt{x}}2,
		\frac{\sqrt{1-x}}y
	\Big)
	\label{eq:player_criterion}
\end{equation}

\textbf{П} стремится минимизировать (выборая $y \in Y = \{1,2\}$)
вектор-функцию выигрыша $F(x, y)$, а игрок \textbf{С} - максимизировать
 (выбирая $x \in X=[0,1]$).

Мы будем рассматривать конечную игру \textbf{C --- П}, полученную из исходной дисеретизацией множества $X$ конечным множеством точек:

$$
	X^T = \{
		0, \; 
		\frac{1}{T}, \; 
		\frac{2}{T}, \; 
		\ldots, \; 
		\frac{T-1}{T}, \;
		1
	\}, \;\; T \in \mathbb{N}
$$

Теперь задачу можно представить в виде многокритериальной игры двух лиц 
с противоположными интересами:


\begin{equation}
	G = \big \langle A, P, \textbf{F} \big \rangle
\end{equation}
	где: 
 
	$ A = $\{\textbf{C}, \textbf{П}\} - множество игроков,

	$ P = \{X^T, \, Y\}$ - множество наборов чистых
	стратегий игроков,
	
	$ \textbf{F} = \{F, \, -F\}$ - множество вектор-функций выигрыша
	игроков.
	
Игра записана в чистых старатегиях. Мы же допустим, что игроки будут 
использовать смешанные стратегии. 
В работе исследуются случаи дискретизации, когда $T=1$ и в этом 
случае множество $X^1=\{0, 1\}$, и когда 
$T=2$, тогда множество чистых стратегий \textbf{С}
примнимет вид $X^2=\{0, \frac{1}{2} ,1\}$.
В каждом случае необходимо решить игру. Составим план, по которому
будет проходить поиск решений \eqref{eq:game_situation}.

\hspace{3mm}

\textbf{(1)} 
Игроки используют смешанные стратегии, поэтому стратегий \textbf{С} и
\textbf{П} будет распределение вероятностей $p \in P$ и 
$q \in Q$ над множествами $X^T$ и $Y$
соответсвенно. Где $P$ и $Q$ -- все допустимые
распределения над этими множествами.
Следовательно заменим вектор-функцию выигрыша каждого
игрока на матожидание этой функции по распределению вероятностей его
стартегии. Для \textbf{С} и \textbf{П} функции принимают вид 
$\mathbb{E}_x [F(x,y)]$ и $\mathbb{E}_y [-F(x,y)]$.

\hspace{3mm}

\textbf{(2)}
Каждый игрок выбирает функцию свёртки, которая будет аппроксимировать его
множество слейтера $S_a$. В данной работе исследуется случай, когда
игрок \textbf{С} выбирет \textit{обратную логическую свёртку}, 
а игрок \textbf{П} \textit{линейную свёртку}. После чего игроки 
применяют свёртку к осреднённой вектор-функции выигрыша.
Для \textbf{С} и \textbf{П} функции принимают вид 
$G(\mathbb{E}_x [F(x,y)], p, y)$ и $L(\mathbb{E}_y [-F(x,y)], x, q)$.

\hspace{3mm}

\textbf{(3)}
Далее игроки получают скалярный критерий, который зависит от 
чистых стратегий противника. Поскольку игроки используют смешанные стратегии,
то на 3 -- ем этапе игроки осредняют скаляризованные критерии по 
стратегиям противника. Мы получили функции выигрыша игроков, которые
зависят от их смешнных стратегий.
Для \textbf{С} и \textbf{П} функции принимают вид 
$$
	\overline G(p, q, \mu) =
	\mathbb{E}_y [G(\mathbb{E}_x [F(x,y)], p, y, \mu)]
$$ и $$
	\overline L(p, q, \lambda) = 
	\mathbb{E}_x [L(\mathbb{E}_y [-F(x,y)], x, q, \lambda)]
$$.

\hspace{3mm}

\textbf{(4)}
Теперь мы получили игру в смешанных стратегиях и нам необходимо решить игру
т.е. найти все возможные равновесия Нэша \eqref{eq:nash_equilibrium}
при фиксированных параметрах свёртки. 
Стратегии, которые являются решениями будем называть 
\textit{оптимальными стратегиями}. В конкретном случае они имеют вид:

\begin{Def}	
	Пара стратегий $(p^0, q^0) \in P \times Q$
	называется оптимальными, если для некоторых 
	$\lambda$, $\mu$ верно:
	\begin{equation}
		\begin{cases} 
			p^0(q^0, \lambda) = 
			\arg \min \limits_{p \in P} \overline L(p, q^0, \lambda) \\ 
			q^0(p^0, \mu) = 
			\arg \min \limits_{q \in Q} \overline G(p^0, q, \mu) \\
		\end{cases}
	\label{def:optimal_strategy}
	\end{equation}
	Введены следующие обозначения:
	\begin{gather*}
		\arg \max \limits_{x \in X} f(x) = 
		\{ x^* \in X \: | \: f(x^*) = \min \limits_{x \in X} f(x)\}
		\\
		\arg \min \limits_{x \in X} f(x) = 
		\{ x^* \in X \: | \: f(x^*) = \max \limits_{x \in X} f(x)\}
	\end{gather*}
\end{Def}


%Рассмотрим случай, когда \textbf{С} использует обратную логическую свертку,
%с парамером $\mu$, a \textbf{П} использует линейную
%свертку с параметром $\lambda$. Тогда множество оптимальных решений
%принимает следующий вид:

%\begin{gather*}
%	\begin{cases}
%		x^*=\arg \max \limits_{x \in X}
%			G(\{f_1, f_2\}, \mu, x, y^*) \\
%		y^*=\arg \min \limits_{y}   
%			L(\{f_1, f_2\}, \lambda, x^*, y)
%	\end{cases}
%\end{gather*}

%Поскольку игроки используют смешанные стратегии т.е. распределения %вероятностей
%$\rho_x(x)$ и $\rho_y(y)$ над чистыми стратегиями $x \in X$ и $y \in Y$. 
%Далее каждый игрок осредняет свою функцию выигрыша по стратегиям противника

%$$ \overline G(\{f_1, f_2\}, \mu, q, p) = 
%\iint \limits_{PQ} G(\{f_1, f_2\}, \mu, q, p) \rho_x(x) \rho_y(y)dxdy$$
%$$ \overline L(\{f_1, f_2\}, \lambda, q, p) = 
%\iint \limits_{PQ} L(\{f_1, f_2\}, \lambda, q, p) \rho_x(x) \rho_y(y)dxdy$